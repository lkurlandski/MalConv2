"""
Modify malware and record the classifier's response to the modified malware.
"""

from pathlib import Path
from pprint import pprint
import multiprocessing
import sys
import typing as tp

import numpy as np
import matplotlib.pyplot as plt
import torch
from torch import Tensor
from tqdm import tqdm

from classifier import (
    CONFIDENCE_THRESHOLD,
    confidence_scores,
    get_model,
    MalConvLike,
    ModelName,
    NUM_EMBEDDINGS,
    PAD_VALUE,
    SOREL_TRAIN_PATH,
    WINDOWS_TRAIN_PATH,
    WINDOWS_TEST_PATH,
)
from config import device
from executable_helper import read_binary, text_section_bounds, text_section_data
from explain import BASELINE
from typing_ import ExeToolkit, Pathlike
from utils import ceil_divide, get_outfile


MAX_INC_SUB_BYTES = 2**24  # 2 ^ 16 = 64 KB, 2 ^ 20 = 1 MB, 2 ^ 24 = 16 MB
BENIGN_FILES = [
    WINDOWS_TRAIN_PATH / f
    for f in [
        "701f928760a612a1e929551ca12363394922f30c7f8181f4df5b0ec0.exe",  # 9.903999e-06 malicious
        "f20a100e661a3179976ccf06ce4a773cbe8d19cd8f50f14e41c0a9e6.exe",  # 3.3748079e-06 malicious
        "09024e62ccab97df3b535e1d65025c54d2d8a684b9e6dcebba79786d.exe",  # 0.9886742 malicious
    ]
] + [
    WINDOWS_TEST_PATH / f
    for f in [
        "05efe7acbe79a7f925c5bc763d11f9d5a1daa2055d297436d0325a1b.exe",  # 1.6685235e-06 malicious
        "256838fe2f037b8865a49d0485c568923f561fea082aa5fa656d6b2d.exe",  # 0.043622814 malicious
        "efe6c4f2299bdc4b89ea935c05c8ebc740937cc7ee4a3948ba552a92.exe",  # 4.975618e-05 malicious
    ]
]


def get_least_suspicious_chunk(
    X: Tensor,
    attributions: Tensor,
    chunk_size: int,
) -> Tensor:
    minimum = attributions.min()
    l = (minimum == attributions).nonzero()[0].item()
    chunk = X[l : l + chunk_size]
    return chunk


def slice_replacement_tensor(
    replacement: Tensor,
    size: int,
    mode: tp.Literal["exact", "truncate", "repeat", "pad"] = "repeat",
) -> Tensor:
    replacement = replacement.clone()
    if mode == "exact":
        replacement = replacement
    elif mode == "truncate":
        replacement = replacement[0:size]
    elif mode == "pad":
        padding = torch.full((size - replacement.shape[0],), PAD_VALUE)
        replacement = torch.cat((replacement, padding), 0)
    elif mode == "repeat":
        num_repeats = ceil_divide(size, replacement.shape[0])
        replacement = torch.cat([replacement for _ in range(num_repeats)], 0)[0:size]
    else:
        raise ValueError(f"Invalid mode: {mode}")
    return replacement


def incremental_substitute(
    model: MalConvLike,
    attributions: Tensor,
    X: Tensor,
    l: int,
    u: int,
    chunk_size: int,
    benign_replacement: tp.Optional[Tensor] = None,
    attribution_threshold: float = 0.0,
    run_baseline: bool = False,
    run_random: bool = False,
    run_benign: bool = False,
) -> tp.Tuple[tp.List[float]]:
    # Set up the return data structures of confidence scores
    baseline_confs, random_confs, benign_confs = [], [], []
    if not any((run_baseline, run_random, run_benign)):
        return baseline_confs, random_confs, benign_confs

    # FIXME: remove
    # np.savetxt(f"inc/complete/{0}.txt", X.numpy(), fmt="%d")
    # np.savetxt(f"inc/text/{0}.txt", X[l:u].numpy(), fmt="%d")

    c = confidence_scores(model, X).item()
    # Populate with the original confidence score
    if run_baseline:
        baseline_confs = [c]
        baseline_X = X.clone()
    if run_random:
        random_confs = [c]
        random_X = X.clone()
    if run_benign:
        benign_confs = [c]
        benign_X = X.clone()

    n_iter = min(
        ceil_divide(attributions.shape[0], chunk_size) + 1,
        MAX_INC_SUB_BYTES // chunk_size,
    )
    for i in range(1, n_iter):
        # If the least suspicious chunk exceeds some threshold, stop
        if (max_attr := attributions.max()) <= attribution_threshold:
            break
        # Get the maximum attribution score and index it first occurs (torch version stable)
        max_attr_lower = (max_attr == attributions).nonzero()[0].item()
        attributions[max_attr_lower : max_attr_lower + chunk_size] = -float("inf")
        # Get the lower and upper bounds and replacement value for the full input tensor
        l_replace = min(u, l + max_attr_lower)
        u_replace = min(u, l + max_attr_lower + chunk_size)
        size = u_replace - l_replace
        # Compute the replacement tensors and new confidence scores
        if run_baseline:
            baseline_X[l_replace:u_replace] = torch.full((size,), BASELINE)
            baseline_confs.append(confidence_scores(model, baseline_X).item())
        if run_random:
            random_X[l_replace:u_replace] = torch.randint(
                low=0, high=NUM_EMBEDDINGS, size=(size,)
            )
            random_confs.append(confidence_scores(model, random_X).item())
        if run_benign:
            benign_X[l_replace:u_replace] = slice_replacement_tensor(
                benign_replacement[max_attr_lower:], size
            )
            benign_confs.append(confidence_scores(model, benign_X).item())

        # FIXME: remove
        # np.savetxt(f"inc/complete/{i}.txt", benign_X.numpy(), fmt="%d")
        # np.savetxt(f"inc/text/{i}.txt", benign_X[l:u].numpy(), fmt="%d")

    return baseline_confs, random_confs, benign_confs


def full_substitute(
    model: MalConvLike,
    X: Tensor,
    l: int,
    u: int,
    benign_replacement: tp.Optional[Tensor] = None,
    mode: tp.Literal["exact", "truncate", "repeat", "pad"] = "repeat",
    run_baseline: bool = False,
    run_random: bool = False,
    run_benign: bool = False,
) -> tp.Tuple[tp.List[float]]:
    # Set up the return data structures of confidence scores
    baseline_confs, random_confs, benign_confs = [], [], []
    if not any((run_baseline, run_random, run_benign)):
        return baseline_confs, random_confs, benign_confs

    # Set up the return data structures of confidence scores
    size = u - l
    c = confidence_scores(model, X).item()

    # Populate with the original confidence score
    if run_baseline:
        X_ = X.clone()
        X_[l:u] = torch.full((size,), BASELINE)
        baseline_confs = [c, confidence_scores(model, X_).item()]
    if run_random:
        X_ = X.clone()
        X_[l:u] = torch.randint(low=0, high=NUM_EMBEDDINGS, size=(size,))
        random_confs = [c, confidence_scores(model, X_).item()]
    if run_benign:
        X_ = X.clone()
        X_[l:u] = slice_replacement_tensor(benign_replacement, size, mode)
        benign_confs = [c, confidence_scores(model, X_).item()]

    return baseline_confs, random_confs, benign_confs


def swaps_count(
    attributions: Tensor,
    chunk_size: int,
    attribution_threshold: float = 0.0,
) -> int:
    n_over_thresh = torch.sum(attributions > attribution_threshold)
    return ceil_divide(n_over_thresh, chunk_size)


def run_per_sample(
    chunk_size: int,
    run_inc_baseline: bool,
    run_inc_random: bool,
    run_inc_benign: bool,
    run_full_baseline: bool,
    run_full_random: bool,
    run_full_benign: bool,
    model: MalConvLike,
    attributions_path: Path,
    confidences_path: Path,
    i: int,
    f: Path,
    l: int,
    u: int,
    benign_replacement: tp.Optional[Tensor] = None,
) -> None:
    attributions_file = attributions_path / f"{f.name}.pt"
    attributions = torch.load(attributions_file, map_location=device)[l:u]
    X = Tensor(read_binary(f))

    inc_baseline, inc_random, inc_benign = incremental_substitute(
        model,
        attributions,
        X,
        l,
        u,
        chunk_size,
        benign_replacement=benign_replacement,
        attribution_threshold=-float("inf"),
        run_baseline=run_inc_baseline,
        run_random=run_inc_random,
        run_benign=run_inc_benign,
    )

    full_baseline, full_random, full_benign = full_substitute(
        model,
        X,
        l,
        u,
        benign_replacement=benign_replacement,
        run_baseline=run_full_baseline,
        run_random=run_full_random,
        run_benign=run_full_benign,
    )

    confs = {
        "inc_baseline": inc_baseline,
        "inc_random": inc_random,
        "inc_benign": inc_benign,
        "full_baseline": full_baseline,
        "full_random": full_random,
        "full_benign": full_benign,
    }
    for k, conf in confs.items():
        if conf:
            p = confidences_path / k / f"{f.name}.txt"
            p.parent.mkdir(exist_ok=True, parents=True)
            np.savetxt(p, conf, delimiter="\n")


def run(
    output_path: Path,
    toolkit: ExeToolkit,
    chunk_size: int,
    skip: tp.Union[tp.Literal[False], str, int] = False,
    n_workers: int = 1,
    run_inc_baseline: bool = False,
    run_inc_random: bool = False,
    run_inc_benign: bool = False,
    run_full_baseline: bool = False,
    run_full_random: bool = False,
    run_full_benign: bool = False,
    run_swaps_count: bool = False,
) -> None:

    model = get_model("two")
    _, _, _, benign_replacement = next(
        text_section_data(BENIGN_FILES[5], toolkit, "torch")
    )

    attributions_path = output_path / "attributions"
    confidences_path = output_path / "confidences" / toolkit
    confidences_path.mkdir(parents=True, exist_ok=True)

    files = SOREL_TRAIN_PATH.iterdir()
    gen = text_section_bounds(
        files, toolkit, min_size=None, max_size=None, errors="ignore"
    )

    # TODO: get multithreading to work
    if n_workers > 1:
        iterable = (
            (
                chunk_size,
                run_inc_baseline,
                run_inc_random,
                run_inc_benign,
                run_full_baseline,
                run_full_random,
                run_full_benign,
                model,
                attributions_path,
                confidences_path,
                i,
                f,
                l,
                u,
                benign_replacement,
            )
            for i, (f, l, u) in enumerate(gen)
        )
        with multiprocessing.Pool(processes=n_workers) as pool:
            pool.map(run_per_sample, iterable)

    else:
        for i, (f, l, u) in enumerate(tqdm(gen, total=1443)):
            if skip == i or skip == f.name:
                skip = False
            if skip:
                continue
            try:
                run_per_sample(
                    chunk_size,
                    run_inc_baseline,
                    run_inc_random,
                    run_inc_benign,
                    run_full_baseline,
                    run_full_random,
                    run_full_benign,
                    model,
                    attributions_path,
                    confidences_path,
                    i,
                    f,
                    l,
                    u,
                    benign_replacement,
                )
            except Exception as e:  # FIXME: remove
                print(f"Error on {f.name}: {e}")
            if run_swaps_count:
                attributions_file = attributions_path / f"{f.name}.pt"
                attributions = torch.load(attributions_file, map_location=device)[l:u]
                threshold = 0.0
                n_swaps = swaps_count(attributions, chunk_size, threshold)
                swaps_count_log = output_path / f"swaps_count_{threshold}.txt"
                with open(swaps_count_log, "a") as handle:
                    handle.write(f"{f.as_posix()}, {n_swaps}\n")
            # sys.exit()  # FIXME: remove


def plot_proportion_negative_vs_swap_absolute(
    data: tp.Dict[str, np.ndarray],
    chunk_size: int,
    mode: tp.Literal["last", "any"],
    **outfile_kwargs,
) -> None:
    lengths = {k: len(confs) for k, confs in data.items()}
    max_iterations = max(lengths.values())
    n_neg_clfs = np.zeros(max_iterations)
    for itr in range(max_iterations):
        for k, confs in sorted(data.items(), key=lambda x: x[0]):
            if mode == "any":
                if any(c < CONFIDENCE_THRESHOLD for c in confs[:itr]):
                    n_neg_clfs[itr] += 1
            elif mode == "last":
                c = confs[itr] if itr < lengths[k] else confs[-1]
                if c < CONFIDENCE_THRESHOLD:
                    n_neg_clfs[itr] += 1
            else:
                raise ValueError(f"Invalid mode: {mode}")

    p_neg_clfs = n_neg_clfs / len(data)

    # Plot one
    fig, ax = plt.subplots()
    ax.plot(chunk_size * np.arange(max_iterations), 100 * p_neg_clfs)
    ax.set_title("False Negative Classification vs Bytes Swapped")
    ax.set_xlabel("Bytes Swapped in .text Section")
    ax.set_ylabel("Percent Classified as Benign")

    if "outfile_stem" not in outfile_kwargs:
        outfile_kwargs["outfile_stem"] = f"proportion_negative_vs_swap_absolute_{mode}"
    if "outfile_suffix" not in outfile_kwargs:
        outfile_kwargs["outfile_suffix"] = ".png"
    fig.savefig(get_outfile(**outfile_kwargs), dpi=400)


def plot_proportion_negative_vs_swap_proportion(
    data: tp.Dict[str, np.ndarray],
    mode: tp.Literal["last", "any"],
    granularity: int = 100,
    **outfile_kwargs,
) -> None:
    n_neg_clfs = np.zeros(granularity)

    for p in range(granularity):
        for k, confs in sorted(data.items(), key=lambda x: x[0]):
            itr = p * len(confs) // granularity
            if mode == "any":
                if any(c < CONFIDENCE_THRESHOLD for c in confs[:itr]):
                    n_neg_clfs[p] += 1
            elif mode == "last":
                c = confs[itr]
                if c < CONFIDENCE_THRESHOLD:
                    n_neg_clfs[p] += 1
            else:
                raise ValueError(f"Invalid mode: {mode}")

    p_neg_clfs = n_neg_clfs / len(data)

    # Plot one
    fig, ax = plt.subplots()
    ax.plot(np.arange(granularity), 100 * p_neg_clfs)
    ax.set_title("False Negative Classification vs Bytes Swapped")
    ax.set_xlabel("Percent Bytes Swapped in .text Section")
    ax.set_ylabel("Percent Classified as Benign")

    if "outfile_stem" not in outfile_kwargs:
        outfile_kwargs[
            "outfile_stem"
        ] = f"proportion_negative_vs_swap_proportion_{mode}"
    if "outfile_suffix" not in outfile_kwargs:
        outfile_kwargs["outfile_suffix"] = ".png"
    fig.savefig(get_outfile(**outfile_kwargs), dpi=400)


def analyze(
    output_path: Path,
    toolkit: ExeToolkit,
    chunk_size: int,
    run_inc_baseline: bool = False,
    run_inc_random: bool = False,
    run_inc_benign: bool = False,
    run_full_baseline: bool = False,
    run_full_random: bool = False,
    run_full_benign: bool = False,
) -> None:
    confidences_path = output_path / "confidences" / toolkit
    analysis_path = confidences_path / "analysis"
    modes_paths = {}
    if run_inc_baseline:
        modes_paths["inc_baseline"] = confidences_path / "inc_baseline"
    if run_inc_random:
        modes_paths["inc_random"] = confidences_path / "inc_random"
    if run_inc_benign:
        modes_paths["inc_benign"] = confidences_path / "inc_benign"
    if run_full_baseline:
        modes_paths["full_baseline"] = confidences_path / "full_baseline"
    if run_full_random:
        modes_paths["full_random"] = confidences_path / "full_random"
    if run_full_benign:
        modes_paths["full_benign"] = confidences_path / "full_benign"
    for m in modes_paths:
        (analysis_path / m).mkdir(parents=True, exist_ok=True)

    avg_conf_diff, prop_cls_flipped = {}, {}
    for m in tqdm(modes_paths):
        files = list(modes_paths[m].iterdir())
        init_tp, init_fp, conf_diff, num_flipped = 0, 0, 0, 0
        confs_data = {}
        for f in tqdm(files, leave=False):
            confs = np.loadtxt(f, delimiter="\n")
            confs = np.expand_dims(confs, axis=0) if not confs.shape else confs
            confs_data[f.stem] = confs
            if len(confs) > 1 and confs[0] > CONFIDENCE_THRESHOLD:
                conf_diff += confs[-1] - confs[0]
                init_tp += 1
                if confs[-1] < CONFIDENCE_THRESHOLD:
                    num_flipped += 1
            else:
                init_fp += 1
        plot_proportion_negative_vs_swap_absolute(
            confs_data,
            chunk_size,
            "any",
            outfile_parent=analysis_path / m,
        )
        plot_proportion_negative_vs_swap_absolute(
            confs_data,
            chunk_size,
            "last",
            outfile_parent=analysis_path / m,
        )
        plot_proportion_negative_vs_swap_proportion(
            confs_data,
            "any",
            outfile_parent=analysis_path / m,
        )
        plot_proportion_negative_vs_swap_proportion(
            confs_data,
            "last",
            outfile_parent=analysis_path / m,
        )
        avg_conf_diff[m] = conf_diff / init_tp
        prop_cls_flipped[m] = num_flipped / init_tp

    return {
        "avg_conf_diff": avg_conf_diff,
        "prop_cls_flipped": prop_cls_flipped,
        "init_tp": init_tp,
        "init_fp": init_fp,
    }


def _verify_inc_and_full_same():
    # Only differences should be in the random method
    root = Path("outputs/7/KernelShap/softmax=False/256/50/1/confidences/pefile")
    paths = {
        "baseline": (root / "inc_baseline", root / "full_baseline"),
        "random": (root / "inc_random", root / "full_random"),
        "benign": (root / "inc_benign", root / "full_benign"),
    }
    data = {k: [] for k in paths}
    for k, (inc_path, full_path) in paths.items():
        for f_inc in inc_path.iterdir():
            f_full = full_path / f_inc.name
            inc = np.loadtxt(f_inc, delimiter="\n")
            full = np.loadtxt(f_full, delimiter="\n")
            if not inc.shape:
                print(f_inc)
            if not full.shape:
                print(f_full)
            inc = np.expand_dims(inc, 0) if not inc.shape else inc
            full = np.expand_dims(full, 0) if not inc.shape else inc
            if inc[-1] != full[-1]:
                data[k].append((f_inc.name, inc, full))
    return data


if __name__ == "__main__":
    chunk_size = 256
    model_name: ModelName = "gct"
    output_path = Path(
        f"outputs/{model_name}/KernelShap/softmax=False/{chunk_size}/50/1/"
    )
    toolkit = "pefile"

    run_inc_baseline = False
    run_inc_random = False
    run_inc_benign = False
    run_full_baseline = False
    run_full_random = False
    run_full_benign = False
    run_swaps_count = False

    # run_inc_baseline = not run_inc_baseline
    # run_inc_random = not run_inc_random
    # run_inc_benign = not run_inc_benign
    # run_full_baseline = not run_full_baseline
    # run_full_random = not run_full_random
    # run_full_benign = not run_full_benign
    # run_swaps_count = not run_swaps_count

    run(
        output_path,
        toolkit,
        chunk_size,
        skip=False,
        n_workers=1,
        run_inc_baseline=run_inc_baseline,
        run_inc_random=run_inc_random,
        run_inc_benign=run_inc_benign,
        run_full_baseline=run_full_baseline,
        run_full_random=run_full_random,
        run_full_benign=run_full_benign,
        run_swaps_count=run_swaps_count,
    )
    results = analyze(
        output_path,
        toolkit,
        chunk_size,
        run_inc_baseline=run_inc_baseline,
        run_inc_random=run_inc_random,
        run_inc_benign=run_inc_benign,
        run_full_baseline=run_full_baseline,
        run_full_random=run_full_random,
        run_full_benign=run_full_benign,
    )
    pprint(results)
