"""

"""

from copy import deepcopy
from pathlib import Path
from pprint import pprint
import multiprocessing
import sys
import typing as tp

import numpy as np
import torch
from torch import Tensor
from tqdm import tqdm

from classifier import (
    confidence_scores,
    get_model,
    MalConvLike,
    MALCONV_PATH,
    SOREL_TRAIN_PATH,
    WINDOWS_TRAIN_PATH,
    WINDOWS_TEST_PATH,
    PAD_VALUE,
)

from config import device
from executable_helper import read_binary, text_section_bounds, text_section_data
from explain import BASELINE
from typing_ import ExeToolkit
from utils import ceil_divide


REPLACE_MODES = ("BASELINE", "RANDOM", "BENIGN_LOOKING", "BENIGN_SWAP_INC", "BENIGN_SWAP")
BENIGN_FILE_1 = (
    WINDOWS_TRAIN_PATH / "09024e62ccab97df3b535e1d65025c54d2d8a684b9e6dcebba79786d.exe"
)  # MalConv2 is 99% confident that this is a malicious file
BENIGN_FILE_2 = (
    WINDOWS_TRAIN_PATH / "f20a100e661a3179976ccf06ce4a773cbe8d19cd8f50f14e41c0a9e6.exe"
)  # MalConv2 is 100% confident that this is a benign file
BENIGN_FILE_3 = (
    WINDOWS_TEST_PATH / "701f928760a612a1e929551ca12363394922f30c7f8181f4df5b0ec0.exe"
)  # MalConv2 is 100% confident that this is a benign file


def slice_replacement_tensor(
        replacement: Tensor,
        size: int,
        mode: tp.Literal["exact", "truncate", "repeat", "pad"] = "repeat",
) -> Tensor:
    replacement = replacement.clone()
    if mode == "exact":
        replacement = replacement
    elif mode == "truncate":
        replacement = replacement[0 : size]
    elif mode == "pad":
        padding = torch.full((size - replacement.shape[0],), PAD_VALUE)
        replacement = torch.cat((replacement, padding), 0)
    elif mode == "repeat":
        num_repeats = ceil_divide(size, replacement.shape[0])
        replacement = torch.cat([replacement for _ in range(num_repeats)], 0)[0 : size]
    else:
        raise ValueError(f"Invalid mode: {mode}")
    return replacement


def get_least_suspicious_chunk(
        X: Tensor,
        attributions: Tensor,
        chunk_size: int,
) -> Tensor:
    minimum = attributions.min()
    l = (minimum == attributions).nonzero()[0].item()
    chunk = X[l: l + chunk_size]
    return chunk


def incremental_substitute(
    model: MalConvLike,
    attributions: Tensor,
    X: Tensor,
    l: int,
    u: int,
    chunk_size: int,
    benign_replacement: tp.Optional[Tensor] = None,
    attribution_threshold: float = 0.0,
    num_embeddings: int = 257,
    run_baseline: bool = False,
    run_random: bool = False,
    run_benign: bool = False,
) -> tp.Tuple[tp.List[float]]:
    # Set up the return data structures of confidence scores
    baseline_confs, random_confs, benign_confs = [], [], []
    if not any((run_baseline, run_random, run_benign)):
        return baseline_confs, random_confs, benign_confs

    c = confidence_scores(model, X).item()
    # Populate with the original confidence score
    if run_baseline:
        baseline_confs = [c]
        baseline_X = X.clone()
    if run_random:
        random_confs = [c]
        random_X = X.clone()
    if run_benign:
        benign_confs = [c]
        benign_X = X.clone()

    for i in range(ceil_divide(attributions.shape[0], chunk_size)):
        # If the least suspicious chunk exceeds some threshold, stop
        if (max_attr := attributions.max()) <= attribution_threshold:
            break
        # Get the maximum attribution score and index it first occurs (torch version stable)
        max_attr_lower = (max_attr == attributions).nonzero()[0].item()
        attributions[max_attr_lower : max_attr_lower + chunk_size] = -float("inf")
        # Get the lower and upper bounds and replacement value for the full input tensor
        l_replace = min(u, l + max_attr_lower)
        u_replace = min(u, l + max_attr_lower + chunk_size)
        size = u_replace - l_replace
        # Compute the replacement tensors and new confidence scores
        if run_baseline:
            baseline_X[l_replace:u_replace] = torch.full((size,), BASELINE)
            baseline_confs.append(confidence_scores(model, baseline_X).item())
        if run_random:
            random_X[l_replace:u_replace] = torch.randint(low=0, high=num_embeddings, size=(size,))
            random_confs.append(confidence_scores(model, random_X).item())
        if run_benign:
            benign_X[l_replace:u_replace] = slice_replacement_tensor(benign_replacement[max_attr_lower:], size)
            benign_confs.append(confidence_scores(model, benign_X).item())

    return baseline_confs, random_confs, benign_confs


def full_substitute(
    model: MalConvLike,
    X: Tensor,
    l: int,
    u: int,
    benign_replacement: tp.Optional[Tensor] = None,
    mode: tp.Literal["exact", "truncate", "repeat", "pad"] = "repeat",
    num_embeddings: int = 257,
    run_baseline: bool = False,
    run_random: bool = False,
    run_benign: bool = False,
) -> tp.Tuple[tp.List[float]]:
    # Set up the return data structures of confidence scores
    baseline_confs, random_confs, benign_confs = [], [], []
    if not any((run_baseline, run_random, run_benign)):
        return baseline_confs, random_confs, benign_confs

    # Set up the return data structures of confidence scores
    size = u - l
    c = confidence_scores(model, X).item()

    # Populate with the original confidence score
    if run_baseline:
        X_ = X.clone()
        X_[l:u] = torch.full((size,), BASELINE)
        baseline_confs = [c, confidence_scores(model, X_).item()]
    if run_random:
        X_ = X.clone()
        X_[l:u] = torch.randint(low=0, high=num_embeddings, size=(size,))
        random_confs = [c, confidence_scores(model, X_).item()]
    if run_benign:
        X_ = X.clone()
        X_[l:u] = slice_replacement_tensor(benign_replacement, size, mode)
        benign_confs = [c, confidence_scores(model, X_).item()]

    return baseline_confs, random_confs, benign_confs


def swaps_count(
        attributions: Tensor,
        chunk_size: int,
        attribution_threshold: float = 0.0,
) -> int:
    n_over_thresh = torch.sum(attributions > attribution_threshold)
    return ceil_divide(n_over_thresh, chunk_size)


def run(
    output_path: Path,
    toolkit: ExeToolkit,
    chunk_size: int,
    skip: tp.Union[tp.Literal[False], str, int] = False,
    run_inc_baseline: bool = False,
    run_inc_random: bool = False,
    run_inc_benign: bool = False,
    run_full_baseline: bool = False,
    run_full_random: bool = False,
    run_full_benign: bool = False,
    run_swaps_count: bool = False,
) -> None:

    model = get_model(MALCONV_PATH)
    _, _, _, benign_replacement = next(text_section_data(BENIGN_FILE_3, toolkit, "torch"))

    attributions_path = output_path / "attributions"
    confidences_path = output_path / "confidences" / toolkit
    confidences_path.mkdir(parents=True, exist_ok=True)

    files = SOREL_TRAIN_PATH.iterdir()
    gen = text_section_bounds(files=files, toolkit=toolkit, errors="ignore")

    for i, (f, l, u) in enumerate(tqdm(gen, total=1443)):
        if f.name == skip or i == skip:
            skip = False
        if skip:
            continue

        attributions_file = attributions_path / f"{f.name}.pt"
        attributions = torch.load(attributions_file, map_location=device)[l:u]
        X = Tensor(read_binary(f))

        inc_baseline, inc_random, inc_benign = incremental_substitute(
            model,
            attributions,
            X,
            l,
            u,
            chunk_size,
            benign_replacement=benign_replacement,
            attribution_threshold=-float("inf"),
            run_baseline=run_inc_baseline,
            run_random=run_inc_random,
            run_benign=run_inc_benign,
        )

        full_baseline, full_random, full_benign = full_substitute(
            model,
            X,
            l,
            u,
            benign_replacement=benign_replacement,
            run_baseline=run_full_baseline,
            run_random=run_full_random,
            run_benign=run_full_benign,
        )

        confs = {
            "inc_baseline": inc_baseline,
            "inc_random": inc_random,
            "inc_benign": inc_benign,
            "full_baseline": full_baseline,
            "full_random": full_random,
            "full_benign": full_benign,
        }
        for k, conf in confs.items():
            if conf:
                p = confidences_path / k / f"{f.name}.txt"
                p.parent.mkdir(exist_ok=True, parents=True)
                np.savetxt(p, conf, delimiter="\n")

        # TODO: determine if this is a multiprocessing performance issue or not
        if run_swaps_count:
            threshold = 0.0
            n_swaps = swaps_count(attributions, chunk_size, threshold)
            swaps_count_log = output_path / f"swaps_count_{threshold}.txt"
            with open(swaps_count_log, "a") as handle:
                handle.write(f"{f.as_posix()}, {n_swaps}\n")


def analyze(
    output_path: Path,
    toolkit: ExeToolkit,
    run_inc_baseline: bool = False,
    run_inc_random: bool = False,
    run_inc_benign: bool = False,
    run_full_baseline: bool = False,
    run_full_random: bool = False,
    run_full_benign: bool = False,
) -> None:

    # TODO: fix analysis
    confidences_path = output_path / "confidences" / toolkit
    modes_paths = {m: confidences_path / m for m in replace_modes}
    avg_conf_diff = {m: 0 for m in replace_modes}
    prop_cls_flipped = {m: 0 for m in replace_modes}

    for m in tqdm(replace_modes):
        files = list(modes_paths[m].iterdir())
        cls_as_benign_count, conf_diff, num_flipped = 0, 0, 0
        for f in tqdm(files, leave=False):
            confs = np.loadtxt(f, delimiter="\n")
            if confs.shape[0] == 1 or confs[0] < 0.5:
                continue  # Skip if file not modified or was classified as benign
            else:
                cls_as_benign_count += 1
            conf_diff += confs[-1] - confs[0]
            if confs[-1] < 0.5:
                num_flipped += 1

        avg_conf_diff[m] = conf_diff / cls_as_benign_count
        prop_cls_flipped[m] = num_flipped / cls_as_benign_count

    return avg_conf_diff, prop_cls_flipped, cls_as_benign_count


def _verify_inc_and_full_same():
    # Only differences should be in the random method
    root = Path("outputs/7/KernelShap/softmax=False/256/50/1/confidences/pefile")
    paths = {
        "baseline": (root / "inc_baseline", root / "full_baseline"),
        "random": (root / "inc_random", root / "full_random"),
        "benign": (root / "inc_benign", root / "full_benign"),
    }
    data = {k : [] for k in paths}
    for k, (inc_path, full_path) in paths.items():
        for f_inc in inc_path.iterdir():
            f_full = full_path / f_inc.name
            inc = np.loadtxt(f_inc, delimiter="\n")[-1]
            full = np.loadtxt(f_full, delimiter="\n")[-1]
            if inc != full:
                data[k].append((f_inc.name, inc, full))
    return data


if __name__ == "__main__":
    chunk_size = 256
    output_path = Path(f"outputs/7/KernelShap/softmax=False/{chunk_size}/50/1/")
    toolkit = "pefile"

    run_inc_baseline = False
    run_inc_random = False
    run_inc_benign = False
    run_full_baseline = False
    run_full_random = False
    run_full_benign = False
    run_swaps_count = False

    run_inc_baseline = not run_inc_baseline
    run_inc_random = not run_inc_random
    run_inc_benign = not run_inc_benign
    run_full_baseline = not run_full_baseline
    run_full_random = not run_full_random
    run_full_benign = not run_full_benign
    run_swaps_count = not run_swaps_count

    run(
        output_path,
        toolkit,
        chunk_size,
        skip=False,
        run_inc_baseline=run_inc_baseline,
        run_inc_random=run_inc_random,
        run_inc_benign=run_inc_benign,
        run_full_baseline=run_full_baseline,
        run_full_random=run_full_random,
        run_full_benign=run_full_benign,
        run_swaps_count=run_swaps_count,
    )
    results = analyze(
        output_path,
        toolkit,
        run_inc_baseline=run_inc_baseline,
        run_inc_random=run_inc_random,
        run_inc_benign_=run_inc_benign,
        run_full_baseline=run_full_baseline,
        run_full_random=run_full_random,
        run_full_benign=run_full_benign,
    )
    pprint(results)
