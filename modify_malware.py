"""

"""

from copy import deepcopy
from pathlib import Path
from pprint import pprint
import sys
import typing as tp

import numpy as np
import torch
from torch import Tensor
from tqdm import tqdm

from classifier import (
    confidence_scores,
    get_model,
    MalConvLike,
    MALCONV_PATH,
    SOREL_TRAIN_PATH,
    WINDOWS_TRAIN_PATH,
    WINDOWS_TEST_PATH,
    PAD_VALUE,
)

from config import device
from executable_helper import read_binary, text_section_bounds, text_section_data
from explain import BASELINE
from typing_ import ExeToolkit
from utils import ceil_divide


REPLACE_MODES = ("BASELINE", "RANDOM", "BENIGN_LOOKING", "BENIGN_SWAP_INC", "BENIGN_SWAP")
BENIGN_FILE_1 = (
    WINDOWS_TRAIN_PATH / "09024e62ccab97df3b535e1d65025c54d2d8a684b9e6dcebba79786d.exe"
)  # MalConv2 is 99% confident that this is a malicious file
BENIGN_FILE_2 = (
    WINDOWS_TRAIN_PATH / "f20a100e661a3179976ccf06ce4a773cbe8d19cd8f50f14e41c0a9e6.exe"
)  # MalConv2 is 100% confident that this is a benign file
BENIGN_FILE_3 = (
    WINDOWS_TEST_PATH / "701f928760a612a1e929551ca12363394922f30c7f8181f4df5b0ec0.exe"
)  # MalConv2 is 100% confident that this is a benign file


def slice_replacement_tensor(
        replacement: Tensor,
        size: int,
        mode: tp.Literal["exact", "truncate", "repeat", "pad"] = "repeat",
) -> Tensor:
    replacement = replacement.clone()
    if mode == "exact":
        replacement = replacement
    elif mode == "truncate":
        replacement = replacement[0 : size]
    elif mode == "pad":
        padding = torch.full((size - replacement.shape[0],), PAD_VALUE)
        replacement = torch.cat((replacement, padding), 0)
    elif mode == "repeat":
        num_repeats = ceil_divide(size, replacement.shape[0])
        replacement = torch.cat([replacement for _ in range(num_repeats)], 0)[0 : size]
    else:
        raise ValueError(f"Invalid mode: {mode}")
    return replacement


def get_least_suspicious_chunk(
        X: Tensor,
        attributions: Tensor,
        chunk_size: int,
) -> Tensor:
    minimum = attributions.min()
    l = (minimum == attributions).nonzero()[0].item()
    chunk = X[0, l: l + chunk_size]
    return chunk

# TODO: function in need of significant refactoring
def incremental_substitute(
    model: MalConvLike,
    attributions: Tensor,
    X: Tensor,
    l: int,
    u: int,
    chunk_size: int,
    replace_modes: tp.Iterable[str],
    benign_replacement: tp.Optional[Tensor] = None,
    attribution_threshold: float = 0.0,
    num_embeddings: int = 257,
) -> tp.Tuple[tp.Dict[str, np.ndarray], int]:
    if "BENIGN_SWAP_INC" in replace_modes and benign_replacement is None:
        raise ValueError("BENIGN_SWAP_INC requires a benign replacement tensor.")
    np.savetxt(f"inc/org/complete.txt", X[0].numpy(), fmt="%i", delimiter="\n")  # FIXME: remove
    np.savetxt(f"inc/org/text.txt", X[0][l:u].numpy(), fmt="%i", delimiter="\n")  # FIXME: remove
    # Methods to replace the byte sequence with
    baseline = torch.full((chunk_size,), BASELINE)
    least_suspicious_chunk = get_least_suspicious_chunk(X, attributions, chunk_size)
    replace_values = {
        "BASELINE": lambda _: baseline,
        "RANDOM": lambda _: torch.randint(low=0, high=num_embeddings, size=(chunk_size,)),
        "BENIGN_LOOKING": lambda _: least_suspicious_chunk,
        "BENIGN_SWAP_INC": lambda _l: slice_replacement_tensor(benign_replacement[_l:], chunk_size),
    }
    replace_values = {
        m: v for m, v in replace_values.items() if m in set(replace_modes)
    }
    # Track the changing confidence scores and the changing input tensor for each substitution mode
    conf = confidence_scores(model, X).item()
    confs = {m: [conf] for m in replace_values}
    replace_X = {m: X.clone() for m in replace_values}
    # Repeatedly alter the malware's .text section and recompute the confidence score
    i, swaps_until_max_attr_neg = 0, 0
    while (max_attr := attributions.max()) > attribution_threshold:
        i += 1
        if max_attr >= 0:
            swaps_until_max_attr_neg += 1
        # Get the maximum attribution score and index it first occurs (torch version stable)
        # then modify the attributions to avoid selecting this region again
        max_attr_lower = (max_attr == attributions).nonzero()[0].item()
        attributions[max_attr_lower : max_attr_lower + chunk_size] = -float("inf")
        for m, replace_func in replace_values.items():
            # Map back to the original vectorized binary and modify the bytes
            size = min(chunk_size, replace_X[m].shape[1] - l - max_attr_lower)
            if size < chunk_size:
                print("DO NOT REMOVE THE STATEMENT ABOVE")
            replace = replace_func(max_attr_lower)[0:size]
            l_replace = l + max_attr_lower
            u_replace = l + max_attr_lower + chunk_size
            if u_replace > u:
                replace = replace[0: chunk_size - (u_replace - u)]
                u_replace = u
            replace_X[m][:, l_replace : u_replace] = replace
            # Recompute confidence score
            conf = confidence_scores(model, replace_X[m]).item()
            confs[m].append(conf)
            np.savetxt(f"inc/new/complete_{i}.txt", replace_X[m][0].numpy(), fmt="%i", delimiter="\n")  # FIXME: remove
            np.savetxt(f"inc/new/text_{i}.txt", replace_X[m][0][l:u].numpy(), fmt="%i", delimiter="\n")  # FIXME: remove
    # Change data type and return
    confs = {m: np.array(c) for m, c in confs.items()}
    return confs, swaps_until_max_attr_neg


# TODO: refactor to use the tensor, not the malware file!
def substitute_with_benign(
    model: MalConvLike,
    malware_file: Path,
    replacement: Tensor,
    toolkit: ExeToolkit,
    mode: tp.Literal["exact", "truncate", "repeat", "pad"] = "repeat",
) -> tp.Dict[str, np.ndarray]:
    # Get the original input, confidence, and text section bounds
    X = Tensor(read_binary(malware_file))
    conf_org = confidence_scores(model, X.unsqueeze(0)).item()
    _, l, u = next(text_section_bounds(malware_file, toolkit))
    # Get the correct size of the replacement tensor
    replacement = slice_replacement_tensor(replacement, u - l, mode)
    # Replace the .text section, get the new confidence score, and return
    np.savetxt(f"full/org/complete.txt", X.numpy(), fmt="%i", delimiter="\n")  # FIXME: remove
    np.savetxt(f"full/org/text.txt", X[l:u].numpy(), fmt="%i", delimiter="\n")  # FIXME: remove
    X[l:u] = replacement
    np.savetxt(f"full/new/complete.txt", X.numpy(), fmt="%i", delimiter="\n")  # FIXME: remove
    np.savetxt(f"full/new/text.txt", X[l:u].numpy(), fmt="%i", delimiter="\n")  # FIXME: remove
    conf_new = confidence_scores(model, X.unsqueeze(0)).item()
    return {"BENIGN_SWAP": np.array([conf_org, conf_new])}


def compute_swaps_until_attribution_threshold_crossed(
        attributions: Tensor,
        chunk_size: int,
        attribution_threshold: float = 0.0,
) -> int:
    n_over_thresh = torch.sum(attributions > attribution_threshold)
    return ceil_divide(n_over_thresh, chunk_size)


def get_replace_modes_flags(
    run_baseline: bool,
    run_random: bool,
    run_benign_looking: bool,
    run_benign_swap_inc: bool,
    run_benign_swap: bool,
) -> tp.Tuple[str]:
    replace_modes = []
    if run_baseline:
        replace_modes.append("BASELINE")
    if run_random:
        replace_modes.append("RANDOM")
    if run_benign_looking:
        replace_modes.append("BENIGN_LOOKING")
    if run_benign_swap_inc:
        replace_modes.append("BENIGN_SWAP_INC")
    if run_benign_swap:
        replace_modes.append("BENIGN_SWAP")
    return tuple(replace_modes)


def run(
    output_path: Path,
    toolkit: ExeToolkit,
    chunk_size: int,
    skip: tp.Union[tp.Literal[False], str, int] = False,
    run_baseline: bool = False,
    run_random: bool = False,
    run_benign_looking: bool = False,
    run_benign_swap_inc: bool = False,
    run_benign_swap: bool = False,
) -> None:
    if not (
        replace_modes := get_replace_modes_flags(
            run_baseline, run_random, run_benign_looking, run_benign_swap_inc, run_benign_swap
        )
    ):
        return

    _, _, _, benign_data = next(text_section_data(BENIGN_FILE_3, toolkit, "torch"))
    model = get_model(MALCONV_PATH)
    # TODO: figure out a way to avoid deleting this file
    swaps_until_max_attr_neg_log = output_path / "log_n_swaps_till_attributions_are_negative.txt"
    # if swaps_until_max_attr_neg_log.exists():
    #     raise Exception("A log file already exists at the output path.")
    swaps_until_max_attr_neg_log.unlink(missing_ok=True)

    attributions_path = output_path / "attributions"
    confidences_path = output_path / "confidences" / toolkit
    modes_paths = {m: confidences_path / m for m in replace_modes}
    for path in modes_paths.values():
        path.mkdir(parents=True, exist_ok=True)

    files = [SOREL_TRAIN_PATH / "001c5a51c1c0d8ac03221caccb768088e171edc3cf4dc4cf473f3878639855b0"]  # SOREL_TRAIN_PATH.iterdir()
    gen = text_section_bounds(files=files, toolkit=toolkit, errors="ignore")
    total = len(deepcopy(gen)) if False else 1443

    for i, (f, l, u) in enumerate(tqdm(gen, total=total)):
        # Skip utility for debugging
        if f.name == skip or i == skip:
            skip = False
        if skip:
            continue

        attributions_file = attributions_path / f"{f.name}.pt"
        attributions = torch.load(attributions_file, map_location=device)
        X = Tensor(read_binary(f)).unsqueeze(0)
        try:
            confs_incremental, swaps_until_max_attr_neg = incremental_substitute(
                model, attributions[l:u], X, l, u, chunk_size, replace_modes, benign_data, -float("inf")
            ) if run_baseline or run_random or run_benign_looking or run_benign_swap_inc else {}
            with open(swaps_until_max_attr_neg_log, "a") as handle:
                handle.write(f"{f.name},{swaps_until_max_attr_neg}\n")
        except Exception as e:
            raise e
            confs_incremental = {}
            s = f"ERROR (incremental).\n\t{f=}\n\t{e=}\n{'-' * 88}\n"
            print(s)
            with open("log_exceptions.txt", "a") as f:
                f.write(s)

        try:
            confs_substitute = substitute_with_benign(model, f, benign_data, toolkit) if run_benign_swap else {}
        except Exception as e:
            confs_substitute = {}
            s = f"ERROR (substitute).\n\t{f=}\n\t{e=}\n{'-' * 88}\n"
            print(s)
            with open("log_exceptions.txt", "a") as f:
                f.write(s)

        confs = {**confs_incremental, **confs_substitute}
        sys.exit()  # FIXME: remove
        for m, c in confs.items():
            np.savetxt(modes_paths[m] / f"{f.name}.txt", c, delimiter="\n")


def analyze(
    output_path: Path,
    toolkit: ExeToolkit,
    run_baseline: bool = False,
    run_random: bool = False,
    run_benign_looking: bool = False,
    run_benign_swap_inc: bool = False,
    run_benign_swap: bool = False,
) -> None:
    if not (
        replace_modes := get_replace_modes_flags(
            run_baseline, run_random, run_benign_looking, run_benign_swap_inc, run_benign_swap
        )
    ):
        return

    confidences_path = output_path / "confidences" / toolkit
    modes_paths = {m: confidences_path / m for m in replace_modes}
    avg_conf_diff = {m: 0 for m in replace_modes}
    prop_cls_flipped = {m: 0 for m in replace_modes}

    for m in tqdm(replace_modes):
        files = list(modes_paths[m].iterdir())
        cls_as_benign_count, conf_diff, num_flipped = 0, 0, 0
        for f in tqdm(files, leave=False):
            confs = np.loadtxt(f, delimiter="\n")
            if confs.shape[0] == 1 or confs[0] < 0.5:
                continue  # Skip if file not modified or was classified as benign
            else:
                cls_as_benign_count += 1
            conf_diff += confs[-1] - confs[0]
            if confs[-1] < 0.5:
                num_flipped += 1

        avg_conf_diff[m] = conf_diff / cls_as_benign_count
        prop_cls_flipped[m] = num_flipped / cls_as_benign_count

    return avg_conf_diff, prop_cls_flipped, cls_as_benign_count


if __name__ == "__main__":
    chunk_size = 256
    output_path = Path(f"outputs/7/KernelShap/softmax=False/{chunk_size}/50/1/")
    toolkit = "pefile"

    run_baseline = False
    run_random = False
    run_benign_looking = False
    run_benign_swap_inc = False
    run_benign_swap = False

    # run_baseline = True
    # run_random = True
    # run_benign_looking = True
    run_benign_swap_inc = True
    run_benign_swap = True

    run(
        output_path,
        toolkit,
        chunk_size,
        skip=False,
        run_baseline=run_baseline,
        run_random=run_random,
        run_benign_looking=run_benign_looking,
        run_benign_swap_inc=run_benign_swap_inc,
        run_benign_swap=run_benign_swap,
    )
    results = analyze(
        output_path,
        toolkit,
        run_baseline=run_baseline,
        run_random=run_random,
        run_benign_looking=run_benign_looking,
        run_benign_swap_inc=run_benign_swap_inc,
        run_benign_swap=run_benign_swap,
    )
    pprint(results)
