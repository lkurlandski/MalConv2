"""

"""

from copy import deepcopy
from itertools import chain
from pathlib import Path
from pprint import pprint
import typing as tp

import numpy as np
import torch
from torch import Tensor
from tqdm import tqdm

from classifier import (
    confidence_scores,
    get_model,
    MalConvLike,
    MALCONV_PATH,
    SOREL_TRAIN_PATH,
    WINDOWS_TRAIN_PATH,
    WINDOWS_TEST_PATH,
    PAD_VALUE,
)

from config import device
from executable_helper import read_binary, text_section_bounds, text_section_data
from explain import BASELINE
from typing_ import ExeToolkit
from utils import ceil_divide


REPLACE_MODES = ("BASELINE", "RANDOM", "BENIGN_LOOKING", "BENIGN_SWAP")
BENIGN_FILE_1 = (
    WINDOWS_TRAIN_PATH / "09024e62ccab97df3b535e1d65025c54d2d8a684b9e6dcebba79786d.exe"
)  # MalConv2 is 99% confident that this is a malicious file
BENIGN_FILE_2 = (
    WINDOWS_TRAIN_PATH / "f20a100e661a3179976ccf06ce4a773cbe8d19cd8f50f14e41c0a9e6.exe"
)  # MalConv2 is 100% confident that this is a benign file
BENIGN_FILE_3 = (
    WINDOWS_TEST_PATH / "701f928760a612a1e929551ca12363394922f30c7f8181f4df5b0ec0.exe"
)  # MalConv2 is 100% confident that this is a benign file


def incremental_substitute(
    model: MalConvLike,
    attributions: Tensor,
    X: Tensor,
    lower: int,
    upper: int,
    chunk_size: int,
    replace_modes: tp.Iterable[str],
    num_embeddings: int = 257,
) -> tp.Dict[str, np.ndarray]:
    def least_suspicious_chunk() -> Tensor:
        minimum = attributions.min()
        lower = (minimum == attributions).nonzero()[0].item()
        chunk = X[0, lower : lower + chunk_size]
        return chunk

    # Isolate for the attributions within the code portion of the malware
    attributions = attributions[lower:upper]
    # Methods to replace the byte sequence with
    baseline = torch.full((chunk_size,), BASELINE)
    least_suspicious_chunk_ = least_suspicious_chunk()
    replace_values = {
        "BASELINE": lambda: baseline,
        "RANDOM": lambda: torch.randint(low=0, high=num_embeddings, size=(chunk_size,)),
        "BENIGN_LOOKING": lambda: least_suspicious_chunk_,
    }
    replace_values = {
        m: v for m, v in replace_values.items() if m in set(replace_modes)
    }
    if not replace_values:
        return {}
    # Track the changing confidence scores and the changing input tensor for each substitution mode
    conf = confidence_scores(model, X).item()
    confs = {m: [conf] for m in replace_values}
    replace_X = {m: X.clone() for m in replace_values}
    # Repeatedly alter the malware's .text section and recompute the confidence score
    while (max_attr := attributions.max()) > 0:
        # Get the maximum attribution score and index it first occurs (torch version stable)
        # then modify the attributions to avoid selecting this region again
        max_attr_lower = (max_attr == attributions).nonzero()[0].item()
        attributions[max_attr_lower : max_attr_lower + chunk_size] = 0
        for m in replace_values:
            # Map back to the original vectorized binary and modify the bytes
            size = min(chunk_size, replace_X[m].shape[1] - lower - max_attr_lower)
            replace = replace_values[m]()[0:size]
            replace_X[m][
                :, lower + max_attr_lower : lower + max_attr_lower + chunk_size
            ] = replace
            # Recompute confidence score
            conf = confidence_scores(model, X).item()
            confs[m].append(conf)
    # Change data type and return
    confs = {m: np.array(c) for m, c in confs.items()}
    return confs


def substitute_with_benign(
    model: MalConvLike,
    malware_file: Path,
    replacement: Tensor,
    toolkit: ExeToolkit,
    mode: tp.Literal["exact", "truncate", "repeat", "pad"] = "repeat",
) -> tp.Dict[str, np.ndarray]:
    # Get the original input, confidence, and text section bounds
    X = Tensor(read_binary(malware_file))
    conf_org = confidence_scores(model, X.unsqueeze(0)).item()
    _, l, u = next(text_section_bounds(malware_file, toolkit))
    # Replace the malware's .text section with the replacement tensor
    replacement = replacement.clone()
    if mode == "exact":
        replacement = replacement
    elif mode == "truncate":
        replacement = replacement[0 : u - l]
    elif mode == "pad":
        padding = torch.full((u - l - replacement.shape[0],), PAD_VALUE)
        replacement = torch.cat((replacement, padding), 0)
    elif mode == "repeat":
        num_repeats = ceil_divide(u - l, replacement.shape[0])
        replacement = torch.cat([replacement for _ in range(num_repeats)], 0)[0 : u - l]
    else:
        raise ValueError(f"Invalid mode: {mode}")
    # Replace the .text section, get the new confidence score, and return
    X[l:u] = replacement
    conf_new = confidence_scores(model, X.unsqueeze(0)).item()
    return {"BENIGN_SWAP": np.array([conf_org, conf_new])}


def get_replace_modes_flags(
    run_baseline: bool,
    run_random: bool,
    run_benign_looking: bool,
    run_benign_swap: bool,
) -> tp.Tuple[str]:
    replace_modes = []
    if run_baseline:
        replace_modes.append("BASELINE")
    if run_random:
        replace_modes.append("RANDOM")
    if run_benign_looking:
        replace_modes.append("BENIGN_LOOKING")
    if run_benign_swap:
        replace_modes.append("BENIGN_SWAP")
    return tuple(replace_modes)


def run(
    output_path: Path,
    toolkit: ExeToolkit,
    chunk_size: int,
    run_baseline: bool = False,
    run_random: bool = False,
    run_benign_looking: bool = False,
    run_benign_swap: bool = False,
) -> None:
    if not (
        replace_modes := get_replace_modes_flags(
            run_baseline, run_random, run_benign_looking, run_benign_swap
        )
    ):
        return

    _, _, _, benign_data = next(text_section_data(BENIGN_FILE_3, toolkit, "torch"))
    model = get_model(MALCONV_PATH)

    attributions_path = output_path / "attributions"
    confidences_path = output_path / "confidences" / toolkit
    modes_paths = {m: confidences_path / m for m in replace_modes}
    for path in modes_paths.values():
        path.mkdir(parents=True, exist_ok=True)

    files = SOREL_TRAIN_PATH.iterdir()
    gen = text_section_bounds(files=files, toolkit=toolkit, errors="ignore")
    total = len(deepcopy(gen)) if False else 1443

    # Use file name to skip processing until that file is encountered
    skip: tp.Union[tp.Literal[False], str] = 298

    for i, (f, l, u) in enumerate(tqdm(gen, total=total)):
        # Skip utility for debugging
        if f.name == skip or i == skip:
            skip = False
        if skip:
            continue

        attributions_file = attributions_path / f"{f.name}.pt"
        attributions = torch.load(attributions_file, map_location=device)
        X = Tensor(read_binary(f)).unsqueeze(0)
        try:
            confs_incremental = incremental_substitute(
                model, attributions, X, l, u, chunk_size, replace_modes
            )
        except Exception as e:
            confs_incremental = {}
            s = f"ERROR (incremental).\n\t{f=}\n\t{e=}\n{'-' * 88}\n"
            print(s)
            with open("log_exceptions.txt", "a") as f:
                f.write(s)

        try:
            confs_substitute = substitute_with_benign(model, f, benign_data, toolkit)
        except Exception as e:
            confs_substitute = {}
            s = f"ERROR (substitute).\n\t{f=}\n\t{e=}\n{'-' * 88}\n"
            print(s)
            with open("log_exceptions.txt", "a") as f:
                f.write(s)

        confs = {**confs_incremental, **confs_substitute}
        for m, c in confs.items():
            np.savetxt(modes_paths[m] / f"{f.name}.txt", c, delimiter="\n")


def analyze(
    output_path: Path,
    toolkit: ExeToolkit,
    run_baseline: bool = False,
    run_random: bool = False,
    run_benign_looking: bool = False,
    run_benign_swap: bool = False,
) -> None:
    if not (
        replace_modes := get_replace_modes_flags(
            run_baseline, run_random, run_benign_looking, run_benign_swap
        )
    ):
        return

    confidences_path = output_path / "confidences" / toolkit
    modes_paths = {m: confidences_path / m for m in replace_modes}
    avg_conf_diff = {m: 0 for m in replace_modes}
    prop_cls_flipped = {m: 0 for m in replace_modes}

    for m in tqdm(replace_modes):
        files = list(modes_paths[m].iterdir())
        cls_as_benign_count, conf_diff, num_flipped = 0, 0, 0
        for f in tqdm(files, leave=False):
            confs = np.loadtxt(f, delimiter="\n")
            if confs.shape[0] == 1 or confs[0] < 0.5:
                continue  # Skip if file not modified or was classified as benign
            else:
                cls_as_benign_count += 1
            conf_diff += confs[-1] - confs[0]
            if confs[-1] < 0.5:
                num_flipped += 1

        avg_conf_diff[m] = conf_diff / cls_as_benign_count
        prop_cls_flipped[m] = num_flipped / cls_as_benign_count

    return avg_conf_diff, prop_cls_flipped, cls_as_benign_count


if __name__ == "__main__":
    chunk_size = 256
    output_path = Path(f"outputs/7/KernelShap/softmax=False/{chunk_size}/50/1/")
    toolkit = "pefile"
    run(
        output_path,
        toolkit,
        chunk_size,
        run_baseline=True,
        run_random=True,
        run_benign_looking=True,
        run_benign_swap=True,
    )
    results = analyze(
        output_path,
        toolkit,
        run_baseline=True,
        run_random=True,
        run_benign_looking=True,
        run_benign_swap=True,
    )
    pprint(results)
